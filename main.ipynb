{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613aeb4b-9dfe-4d58-a373-0d7cd2997144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import datasets\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "task = \"srl\"\n",
    "model_checkpoint = \"/home/ziggy/Desktop/Ad_ml/final/base_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1827817c-4f72-43d6-a463-3c0f33a5e266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ziggy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import data_process\n",
    "from data_process import calculate_failure_rate, classify_data, extract_feature_and_label,preprocess_data_model, read_conll_file, write_conll_multiple, tokenize_conll, tokenize_, format_conll, format_conll, preprocess_data,create_word_sentlist,tokenize_and_align_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bbf7da-ee82-47a8-a112-2d148813e60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from feature import tokenize, lemmatization, named_entity_recognition, sub_tree, capitalization, syntactic_head, PoS_tag, Tag, dep_relations, dep_path, dep_dist_to_head, extract_morph_inform, is_predicate, extract_wordnet_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adb8e8-d5fd-4f5a-90bd-d0c9a7ee7b4d",
   "metadata": {},
   "source": [
    "# Read from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8077dc00-10ad-44fa-9d9b-cac591b6dbd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open the JSON file\n",
    "with open('data_5.json') as f:\n",
    "    \n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711b81a-d1ca-4dfd-ae5a-90004e7107f0",
   "metadata": {},
   "source": [
    "## Extract tests and target information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a8270c-b0c6-4c3f-9003-4544d5a9012f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract text, token, label, pred from json\n",
    "\n",
    "test_sentence={}\n",
    "t_target={}\n",
    "capability_sets=list(data['capabilities'].keys())\n",
    "# print(capability_sets)\n",
    "\n",
    "for c in capability_sets:\n",
    "\n",
    "    # c_name = data['capabilities'][c]['name']\n",
    "\n",
    "    \n",
    "    c_test = data['capabilities'][c][\"tests\"]\n",
    "    # print(c_test)\n",
    "\n",
    "    for t_name in list(c_test.keys()):\n",
    "        # print(t_name)\n",
    "        l_dict = c_test[t_name]\n",
    "    \n",
    "        if not isinstance(l_dict, list):\n",
    "\n",
    "            l_dict = list(c_test[t_name].values())[0]\n",
    "            t_name =list(c_test[t_name].keys())[0]\n",
    "                \n",
    "            # print(t_name, l_dict)\n",
    "            \n",
    "            \n",
    "        for dic in l_dict:\n",
    "            \n",
    "            if not isinstance(dic[\"data\"], list):\n",
    "                # print(dic[\"data\"])\n",
    "                \n",
    "                if t_name in test_sentence:\n",
    "                    test_sentence[t_name].append(dic[\"data\"])\n",
    "                    t_target[t_name].append((dic[\"token\"], dic[\"label\"], dic[\"predicate\"]))\n",
    "                else:\n",
    "                    test_sentence[t_name] = [dic[\"data\"]]\n",
    "\n",
    "                    t_target[t_name]= [(dic[\"token\"], dic[\"label\"], dic[\"predicate\"])]\n",
    "            else:\n",
    "                # print(dic[\"data\"])\n",
    "                for i in range(len(dic[\"data\"])):\n",
    "                    text, token, label, pred = dic[\"data\"][i], dic[\"token\"][i], dic[\"label\"][i], dic[\"predicate\"][i]\n",
    "                    # print(data, token, label, pred)\n",
    "                    \n",
    "                    if t_name in test_sentence:\n",
    "                        test_sentence[t_name].append(text)\n",
    "                        t_target[t_name].append((token, label, pred))\n",
    "                    else:\n",
    "                        test_sentence[t_name] = [text]\n",
    "\n",
    "                        t_target[t_name]= [(token, label, pred)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb477371-cb88-47d8-846c-0ea6757d7a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['passive', 'relative_cause', 'indirect_object', 'double_object', 'Location_modifier', 'Negation'])\n",
      "dict_keys(['passive', 'relative_cause', 'indirect_object', 'double_object', 'Location_modifier', 'Negation'])\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence.keys())\n",
    "print(t_target.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8254fd5d-8502-4748-a574-b289d951f61e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['passive', 'relative_cause', 'indirect_object', 'double_object', 'Location_modifier', 'Negation']\n"
     ]
    }
   ],
   "source": [
    "test_names=list(t_target.keys())\n",
    "print(test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904ed616-a13a-48bf-9502-4a5c9ce3fc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('play', 'ARG1', 'directed'), ('project', 'ARG1', 'managed'), ('movie', 'ARG1', 'made'), ('painting', 'ARG1', 'created'), ('problem', 'ARG1', 'solved'), ('dance', 'ARG1', 'choreographed'), ('building', 'ARG1', 'designed'), ('surgery', 'ARG1', 'performed'), ('speech', 'ARG1', 'delivered')]\n"
     ]
    }
   ],
   "source": [
    "print(t_target[\"passive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c0a33-337d-4aa3-9777-e8337cfb6807",
   "metadata": {},
   "source": [
    "# 1. Tests for Passive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cd052ac-283b-4d46-bbeb-9c2dcff4ce8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name = \"passive\"\n",
    "caba_sents = test_sentence[test_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b06028-bc18-479a-b759-e1e6b93274aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ad718-69c6-407b-9456-8ee2fd24fdc0",
   "metadata": {},
   "source": [
    "## 1.1 Extract Features and data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fb0a50f-7bfb-43f4-9e42-4e3938a9363c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create features and write into conll for inspection\n",
    "output_file = f\"{test_name}_f.conll\"\n",
    "write_conll_multiple(caba_sents, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c453c88-2358-4dd0-ab9e-2224dced3106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read conll\n",
    "conll_output = read_conll_file(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0d7bc2c-6749-46c0-9a88-c14b27b93353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "preprocessed_test=preprocess_data_model(conll_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9376b6-7622-4f94-8ad8-b10a0e5dd213",
   "metadata": {},
   "source": [
    "## 1.2 Implement logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "defc8767-4bae-49d0-8187-f65e0016408d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziggy/anaconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.3.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ziggy/anaconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DictVectorizer from version 1.3.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model and vectorizer\n",
    "model_single = joblib.load('logistic_regression/logreg_single.pkl')\n",
    "vec_single = joblib.load('logistic_regression/vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367fd130-2c60-4a1d-a16b-cf4abf0391db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test,t_tokens,t_label)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a8dbb-d99e-42ec-934b-a52a640eaaa7",
   "metadata": {},
   "source": [
    "## 1.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f44dad-0359-4729-9f53-cc6f89e4d7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#target token\n",
    "t_token_label = []\n",
    "t_token_pred = []\n",
    "for i,t in enumerate(test_gold):\n",
    "    if t != \"_\":\n",
    "        t_token_label.append(t)\n",
    "        t_token_pred.append(single_predictions[i])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d395a1-46fb-4531-be21-d141f23345f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33ccdd4b-71ae-4f9a-b1bd-bbf82c1c58f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9)}\n"
     ]
    }
   ],
   "source": [
    "input_json = {}\n",
    "if test_name not in input_json:\n",
    "    input_json[test_name] = ((failure_rate, failure_count))\n",
    "    \n",
    "print(input_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8849930-7c4d-48db-8744-bc6b57df2ae0",
   "metadata": {},
   "source": [
    "# 2. Tests for Relative_cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7dd767c-e80e-42a9-be77-d1fc00556cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name = \"relative_cause\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "#create features and write into conll for inspection\n",
    "output_file = f\"{test_name}_f.conll\"\n",
    "write_conll_multiple(caba_sents, output_file)\n",
    "\n",
    "#read conll\n",
    "conll_output = read_conll_file(output_file)\n",
    "\n",
    "#preprocess data\n",
    "preprocessed_test=preprocess_data_model(conll_output)\n",
    "\n",
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test,t_tokens,t_label)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)\n",
    "\n",
    "#target token\n",
    "t_token_label = []\n",
    "t_token_pred = []\n",
    "for i,t in enumerate(test_gold):\n",
    "    if t != \"_\":\n",
    "        t_token_label.append(t)\n",
    "        t_token_pred.append(single_predictions[i])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json:\n",
    "    input_json [test_name]= ((failure_rate, failure_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a24ac5bf-7d7f-4946-ae9a-d44f1b2d3179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (1.0, 9)}\n"
     ]
    }
   ],
   "source": [
    "print(input_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a66f2-483f-4d1c-80e1-64655882bb5b",
   "metadata": {},
   "source": [
    "# 3. Tests for indirect_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd43b426-14e2-4389-97bc-669f597f1974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name = \"indirect_object\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "#create features and write into conll for inspection\n",
    "output_file = f\"{test_name}_f.conll\"\n",
    "write_conll_multiple(caba_sents, output_file)\n",
    "\n",
    "#read conll\n",
    "conll_output = read_conll_file(output_file)\n",
    "\n",
    "#preprocess data\n",
    "preprocessed_test=preprocess_data_model(conll_output)\n",
    "\n",
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test,t_tokens,t_label)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)\n",
    "\n",
    "#target token\n",
    "t_token_label = []\n",
    "t_token_pred = []\n",
    "for i,t in enumerate(test_gold):\n",
    "    if t != \"_\":\n",
    "        t_token_label.append(t)\n",
    "        t_token_pred.append(single_predictions[i])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json:\n",
    "    input_json [test_name]= ((failure_rate, failure_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a06058e-5d94-45f0-9103-0cd2fdeb0ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (1.0, 9), 'indirect_object': (1.0, 9)}\n"
     ]
    }
   ],
   "source": [
    "print(input_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f76f61-df2c-44f1-94e5-778fffb4f8ea",
   "metadata": {},
   "source": [
    "# 4. Tests for double_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df26e561-0e1f-4839-acea-ce12fc0f9f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (1.0, 9), 'indirect_object': (1.0, 9), 'double_object': (1.0, 10)}\n"
     ]
    }
   ],
   "source": [
    "test_name = \"double_object\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "#create features and write into conll for inspection\n",
    "output_file = f\"{test_name}_f.conll\"\n",
    "write_conll_multiple(caba_sents, output_file)\n",
    "\n",
    "#read conll\n",
    "conll_output = read_conll_file(output_file)\n",
    "\n",
    "#preprocess data\n",
    "preprocessed_test=preprocess_data_model(conll_output)\n",
    "\n",
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test,t_tokens,t_label)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)\n",
    "\n",
    "#target token\n",
    "t_token_label = []\n",
    "t_token_pred = []\n",
    "for i,t in enumerate(test_gold):\n",
    "    if t != \"_\":\n",
    "        t_token_label.append(t)\n",
    "        t_token_pred.append(single_predictions[i])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json:\n",
    "    input_json [test_name]= ((failure_rate, failure_count))\n",
    "\n",
    "print(input_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db7bee42-88c3-46dd-85f1-5bb272fb3b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARG2', 'ARG2', 'ARG2', 'ARG2', 'ARG2', 'ARG2', 'ARG2', 'ARG2', 'ARG2', 'ARG2']\n"
     ]
    }
   ],
   "source": [
    "print(t_token_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d6173-12fd-4f11-95d7-be6d86b144b3",
   "metadata": {},
   "source": [
    "# 5. Tests for Location_modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caac2f67-66c0-4692-9c87-517caaf1ce62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (1.0, 9), 'indirect_object': (1.0, 9), 'double_object': (1.0, 10), 'Location_modifier': (1.0, 16)}\n"
     ]
    }
   ],
   "source": [
    "test_name = \"Location_modifier\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "#create features and write into conll for inspection\n",
    "output_file = f\"{test_name}_f.conll\"\n",
    "write_conll_multiple(caba_sents, output_file)\n",
    "\n",
    "#read conll\n",
    "conll_output = read_conll_file(output_file)\n",
    "\n",
    "#preprocess data\n",
    "preprocessed_test=preprocess_data_model(conll_output)\n",
    "\n",
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test,t_tokens,t_label)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)\n",
    "\n",
    "#target token\n",
    "t_token_label = []\n",
    "t_token_pred = []\n",
    "for i,t in enumerate(test_gold):\n",
    "    if t != \"_\":\n",
    "        t_token_label.append(t)\n",
    "        t_token_pred.append(single_predictions[i])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json:\n",
    "    input_json [test_name]= ((failure_rate, failure_count))\n",
    "\n",
    "print(input_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3c3a6-7f83-4950-beef-ca7804cec2ee",
   "metadata": {},
   "source": [
    "# 6. Tests for Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a50236b-8864-470f-b4dd-dd0316ca2ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (1.0, 9), 'indirect_object': (1.0, 9), 'double_object': (1.0, 10), 'Location_modifier': (1.0, 16), 'Negation': (1.0, 8)}\n"
     ]
    }
   ],
   "source": [
    "test_name = \"Negation\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "#create features and write into conll for inspection\n",
    "output_file = f\"{test_name}_f.conll\"\n",
    "write_conll_multiple(caba_sents, output_file)\n",
    "\n",
    "#read conll\n",
    "conll_output = read_conll_file(output_file)\n",
    "\n",
    "#preprocess data\n",
    "preprocessed_test=preprocess_data_model(conll_output)\n",
    "\n",
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test,t_tokens,t_label)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)\n",
    "\n",
    "#target token\n",
    "t_token_label = []\n",
    "t_token_pred = []\n",
    "for i,t in enumerate(test_gold):\n",
    "    if t != \"_\":\n",
    "        t_token_label.append(t)\n",
    "        t_token_pred.append(single_predictions[i])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json:\n",
    "    input_json [test_name]= ((failure_rate, failure_count))\n",
    "\n",
    "print(input_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a13e4aef-8a94-4882-8ae3-b181b2df9cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_data_with_test(input_json, input_file):\n",
    "    # Load the JSON file\n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Update data with test information\n",
    "    \n",
    "    for k in input_json: \n",
    "        test_name = k\n",
    "        failure_rate, failure_count = input_json[k]\n",
    "        for capability in data['capabilities'].values():\n",
    "        # print(capability)\n",
    "            tests = capability.get(\"tests\")\n",
    "            # print(tests)\n",
    "            if len(tests) == 1:\n",
    "                data_name = [k for k in tests][0]\n",
    "\n",
    "                if data_name == test_name:\n",
    "                    # capability['failure_rate'] = capability.pop('pass')\n",
    "                    capability['failure_rate'] = failure_rate\n",
    "                    capability['fail'] = failure_count\n",
    "                    # print(capability['fail'])\n",
    "            else:\n",
    "                for i in tests:\n",
    "                    # print(tests[i])\n",
    "                    # print(i)\n",
    "                    data_name = [k for k in tests[i]][0]\n",
    "\n",
    "                    if data_name == test_name:\n",
    "                        # capability['failure_rate'] = capability.pop('pass')\n",
    "                        capability['tests'][i][\"failure_rate\"] = failure_rate\n",
    "                        capability['tests'][i][\"fail\"] = failure_count\n",
    "\n",
    "    # Write the updated data back to the file\n",
    "    with open('data_result.json', 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91e0df5e-71a1-4592-b0c3-df2b8c1e8836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data_5.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for k in input_json_a: \n",
    "    test_name = k\n",
    "    failure_rate, failure_count = input_json_a[k]\n",
    "    for capability in data['capabilities'].values():\n",
    "        # print(capability)\n",
    "        tests = capability.get(\"tests\")\n",
    "            # print(tests)\n",
    "        if len(tests) == 1:\n",
    "            data_name = [k for k in tests][0]\n",
    "\n",
    "            if data_name == test_name:\n",
    "                    # capability['failure_rate'] = capability.pop('pass')\n",
    "                \n",
    "                capability['failure_rate'] = failure_rate\n",
    "                \n",
    "                capability['fail'] = failure_count\n",
    "                    # print(capability['fail'])\n",
    "        else:\n",
    "            for i in tests:\n",
    "                # print(tests[i])\n",
    "                # print(i)\n",
    "                data_name = [k for k in tests[i]][0]\n",
    "\n",
    "                if data_name == test_name:\n",
    "                    # print(\"right name\")\n",
    "                        # capability['failure_rate'] = capability.pop('pass')\n",
    "                    \n",
    "                    tests[i][\"failure_rate\"] = failure_rate\n",
    "                    \n",
    "                    tests[i][\"fail\"] = failure_count\n",
    "\n",
    "with open('data_result.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b699a5c-c973-4697-98db-3c946b1e6360",
   "metadata": {},
   "source": [
    "# 2. Advance_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7652eaed-dd46-43bb-b2ab-bd3680e2b317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {'C-ARGM-EXT': 0, 'C-ARG4': 1, 'C-ARGM-DIR': 2, 'ARG5': 3, 'C-ARGM-CXN': 4, 'ARGA': 5, 'C-ARG2': 6, 'C-ARGM-GOL': 7, 'C-V': 8, 'ARGM-MNR': 9, 'R-ARGM-TMP': 10, 'ARGM-LOC': 11, 'ARGM-DIR': 12, 'C-ARGM-TMP': 13, 'C-ARG3': 14, 'C-ARGM-COM': 15, 'ARGM-ADV': 16, '_': 17, 'R-ARGM-GOL': 18, 'C-ARGM-ADV': 19, 'R-ARGM-ADV': 20, 'R-ARG1': 21, 'ARGM-CAU': 22, 'C-ARGM-PRR': 23, 'ARG3': 24, 'C-ARG1-DSP': 25, 'R-ARGM-CAU': 26, 'C-ARGM-LOC': 27, 'R-ARG0': 28, 'R-ARG3': 29, 'ARG1': 30, 'R-ARGM-LOC': 31, 'ARGM-GOL': 32, 'ARGM-DIS': 33, 'ARGM-PRD': 34, 'C-ARG1': 35, 'R-ARGM-MNR': 36, 'ARGM-EXT': 37, 'ARG2': 38, 'ARGM-TMP': 39, 'R-ARG2': 40, 'R-ARG4': 41, 'ARG0': 42, 'ARGM-PRR': 43, 'R-ARGM-DIR': 44, 'ARG1-DSP': 45, 'ARGM-CXN': 46, 'ARGM-PRP': 47, 'C-ARG0': 48, 'C-ARGM-PRP': 49, 'R-ARGM-COM': 50, 'ARGM-REC': 51, 'R-ARGM-ADJ': 52, 'C-ARGM-MNR': 53, 'ARGM-NEG': 54, 'ARGM-COM': 55, 'ARGM-ADJ': 56, 'ARGM-MOD': 57, 'ARG4': 58, 'ARGM-LVB': 59}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "376f47c3-9c5b-4039-b568-910247b2bcd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load machine\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "import torch\n",
    "\n",
    "model_directory = 'advanced_model/advanced'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_directory,num_labels=len(label_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169695a-b67d-4cf0-a0eb-dcd528c2549d",
   "metadata": {},
   "source": [
    "# 2.1 passive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12ba0a52-74ab-4a67-b723-f3956812e1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_name = \"passive\"\n",
    "# test_sents = test_sentence[test_name]\n",
    "# target_infor = t_target[test_name]\n",
    "# tokenized_sentences = tokenize_conll(test_sents)\n",
    "\n",
    "# passive_ds = preprocess_data(tokenized_sentences,target_infor)\n",
    "# # print(passive_ds)\n",
    "# p_testsent = create_word_sentlist(passive_ds)\n",
    "# p_testds = datasets.Dataset.from_list(p_testsent)\n",
    "# p_tokenized_test = p_testds.map(tokenize_and_align_labels, batched=True)\n",
    "# # print(p_tokenized_test)\n",
    "# p_testds = datasets.Dataset.from_list(p_testsent)\n",
    "# p_tokenized_test = p_testds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4577bc6a-0751-4028-8f36-e68548c1d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"passive\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c4afab5-c9d8-476c-abb0-43433a833f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(caba_sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Decode predictions using label_dict\n",
    "decoded_predictions = []\n",
    "for i, sentence_predictions in enumerate(predictions):\n",
    "    decoded_labels = [list(label_dict.keys())[list(label_dict.values()).index(label_id)] for label_id in sentence_predictions]\n",
    "    decoded_predictions.append(decoded_labels)\n",
    "\n",
    "t_token_label=[]\n",
    "t_token_pred=[]\n",
    "# Print decoded predictions\n",
    "for i, sentence in enumerate(caba_sents):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[i])\n",
    "    t_token_label.append(t_label[i])\n",
    "    t_token = t_tokens[i]\n",
    "    ind_pred = tokens.index(t_token)\n",
    "    t_token_pred.append(decoded_predictions[ind_pred])\n",
    "    # print(\"Sentence:\", sentence)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    # print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35e8fb6e-37bb-470b-b6a2-951323b006d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9)}\n"
     ]
    }
   ],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "# print(failure_rate, failure_count)\n",
    "input_json_a = {}\n",
    "if test_name not in input_json_a:\n",
    "    input_json_a[test_name] = ((failure_rate, failure_count))\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f6d96-c5e3-4f65-b356-b4423f0ece05",
   "metadata": {},
   "source": [
    "# 2.2 relative_cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe42a33-b059-49bb-9884-edf2f21963fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m t_token \u001b[38;5;241m=\u001b[39m t_tokens[i]\n\u001b[1;32m     30\u001b[0m ind_pred \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mindex(t_token)\n\u001b[0;32m---> 31\u001b[0m t_token_pred\u001b[38;5;241m.\u001b[39mappend(decoded_predictions[ind_pred])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(\"Sentence:\", sentence)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"Tokens:\", tokens)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"Predicted Labels:\", decoded_predictions[i])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_name = \"relative_cause\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(caba_sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Decode predictions using label_dict\n",
    "decoded_predictions = []\n",
    "for i, sentence_predictions in enumerate(predictions):\n",
    "    decoded_labels = [list(label_dict.keys())[list(label_dict.values()).index(label_id)] for label_id in sentence_predictions]\n",
    "    decoded_predictions.append(decoded_labels)\n",
    "\n",
    "t_token_label=[]\n",
    "t_token_pred=[]\n",
    "# Print decoded predictions\n",
    "for i, sentence in enumerate(caba_sents):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[i])\n",
    "    t_token_label.append(t_label[i])\n",
    "    t_token = t_tokens[i]\n",
    "    ind_pred = tokens.index(t_token)\n",
    "    t_token_pred.append(decoded_predictions[ind_pred])\n",
    "    # print(\"Sentence:\", sentence)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    # print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74c1fc74-4fd9-48e1-9a7c-78842a306cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (0.75, 3)}\n"
     ]
    }
   ],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json_a:\n",
    "    input_json_a[test_name] = ((failure_rate, failure_count))\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557797d-640d-429f-9cb7-da2017879af6",
   "metadata": {},
   "source": [
    "# 2.3 indirect_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94cc465c-c9f0-4178-9a03-5771772d8d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'Mindy' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m t_token_label\u001b[38;5;241m.\u001b[39mappend(t_label[i])\n\u001b[1;32m     29\u001b[0m t_token \u001b[38;5;241m=\u001b[39m t_tokens[i]\n\u001b[0;32m---> 30\u001b[0m ind_pred \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mindex(t_token)\n\u001b[1;32m     31\u001b[0m t_token_pred\u001b[38;5;241m.\u001b[39mappend(decoded_predictions[ind_pred])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(\"Sentence:\", sentence)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"Tokens:\", tokens)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"Predicted Labels:\", decoded_predictions[i])\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'Mindy' is not in list"
     ]
    }
   ],
   "source": [
    "test_name = \"indirect_object\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(caba_sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Decode predictions using label_dict\n",
    "decoded_predictions = []\n",
    "for i, sentence_predictions in enumerate(predictions):\n",
    "    decoded_labels = [list(label_dict.keys())[list(label_dict.values()).index(label_id)] for label_id in sentence_predictions]\n",
    "    decoded_predictions.append(decoded_labels)\n",
    "\n",
    "t_token_label=[]\n",
    "t_token_pred=[]\n",
    "# Print decoded predictions\n",
    "for i, sentence in enumerate(caba_sents):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[i])\n",
    "    t_token_label.append(t_label[i])\n",
    "    t_token = t_tokens[i]\n",
    "    ind_pred = tokens.index(t_token)\n",
    "    t_token_pred.append(decoded_predictions[ind_pred])\n",
    "    # print(\"Sentence:\", sentence)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    # print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "438e8b8b-2169-489c-92b3-6dfa33cd9230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (0.75, 3), 'indirect_object': (0.5, 1)}\n"
     ]
    }
   ],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json_a:\n",
    "    input_json_a[test_name] = ((failure_rate, failure_count))\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469709f-d671-4e5f-90e8-b2d888870a94",
   "metadata": {},
   "source": [
    "# 2.4 double_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e549bc04-cc3f-4b91-b82a-9eec41e5ebd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'Mindy' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m t_token_label\u001b[38;5;241m.\u001b[39mappend(t_label[i])\n\u001b[1;32m     29\u001b[0m t_token \u001b[38;5;241m=\u001b[39m t_tokens[i]\n\u001b[0;32m---> 30\u001b[0m ind_pred \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mindex(t_token)\n\u001b[1;32m     31\u001b[0m t_token_pred\u001b[38;5;241m.\u001b[39mappend(decoded_predictions[ind_pred])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(\"Sentence:\", sentence)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"Tokens:\", tokens)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"Predicted Labels:\", decoded_predictions[i])\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'Mindy' is not in list"
     ]
    }
   ],
   "source": [
    "test_name = \"double_object\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(caba_sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Decode predictions using label_dict\n",
    "decoded_predictions = []\n",
    "for i, sentence_predictions in enumerate(predictions):\n",
    "    decoded_labels = [list(label_dict.keys())[list(label_dict.values()).index(label_id)] for label_id in sentence_predictions]\n",
    "    decoded_predictions.append(decoded_labels)\n",
    "\n",
    "t_token_label=[]\n",
    "t_token_pred=[]\n",
    "# Print decoded predictions\n",
    "for i, sentence in enumerate(caba_sents):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[i])\n",
    "    t_token_label.append(t_label[i])\n",
    "    t_token = t_tokens[i]\n",
    "    ind_pred = tokens.index(t_token)\n",
    "    t_token_pred.append(decoded_predictions[ind_pred])\n",
    "    # print(\"Sentence:\", sentence)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    # print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d85230a-dfc5-49fa-aa96-1332f65ac112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (0.75, 3), 'indirect_object': (0.5, 1), 'double_object': (0.5, 1)}\n"
     ]
    }
   ],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json_a:\n",
    "    input_json_a[test_name] = ((failure_rate, failure_count))\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08f4e2-27a6-4e95-ba24-3bcee4ff90cf",
   "metadata": {},
   "source": [
    "# 2.5 Location_modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00ef1a3d-986a-421f-9c5b-4ffaa435d38c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'China' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m t_token_label\u001b[38;5;241m.\u001b[39mappend(t_label[i])\n\u001b[1;32m     29\u001b[0m t_token \u001b[38;5;241m=\u001b[39m t_tokens[i]\n\u001b[0;32m---> 30\u001b[0m ind_pred \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mindex(t_token)\n\u001b[1;32m     31\u001b[0m t_token_pred\u001b[38;5;241m.\u001b[39mappend(decoded_predictions[ind_pred])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(\"Sentence:\", sentence)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"Tokens:\", tokens)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"Predicted Labels:\", decoded_predictions[i])\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'China' is not in list"
     ]
    }
   ],
   "source": [
    "test_name = \"Location_modifier\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(caba_sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Decode predictions using label_dict\n",
    "decoded_predictions = []\n",
    "for i, sentence_predictions in enumerate(predictions):\n",
    "    decoded_labels = [list(label_dict.keys())[list(label_dict.values()).index(label_id)] for label_id in sentence_predictions]\n",
    "    decoded_predictions.append(decoded_labels)\n",
    "\n",
    "t_token_label=[]\n",
    "t_token_pred=[]\n",
    "# Print decoded predictions\n",
    "for i, sentence in enumerate(caba_sents):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[i])\n",
    "    t_token_label.append(t_label[i])\n",
    "    t_token = t_tokens[i]\n",
    "    ind_pred = tokens.index(t_token)\n",
    "    t_token_pred.append(decoded_predictions[ind_pred])\n",
    "    # print(\"Sentence:\", sentence)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    # print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e38f6820-15bf-4f1d-a6cd-8dc099d3c0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (0.75, 3), 'indirect_object': (0.5, 1), 'double_object': (0.5, 1), 'Location_modifier': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "if test_name not in input_json_a:\n",
    "    input_json_a[test_name] = ((failure_rate, failure_count))\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1891c-45a1-4fff-8799-83cb763988f0",
   "metadata": {},
   "source": [
    "# 2.6 Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e071361c-bbb3-4139-911a-73b80a02e6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'p' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m t_token_label\u001b[38;5;241m.\u001b[39mappend(t_label[i])\n\u001b[1;32m     29\u001b[0m t_token \u001b[38;5;241m=\u001b[39m t_tokens[i]\n\u001b[0;32m---> 30\u001b[0m ind_pred \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mindex(t_token)\n\u001b[1;32m     31\u001b[0m t_token_pred\u001b[38;5;241m.\u001b[39mappend(decoded_predictions[ind_pred])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(\"Sentence:\", sentence)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"Tokens:\", tokens)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"Predicted Labels:\", decoded_predictions[i])\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'p' is not in list"
     ]
    }
   ],
   "source": [
    "test_name = \"Negation\"\n",
    "caba_sents = test_sentence[test_name]\n",
    "t_tokens = [tup[0] for tup in t_target[test_name]] \n",
    "t_label= [tup[1] for tup in t_target[test_name]] \n",
    "t_pred  = [tup[2] for tup in t_target[test_name]] \n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(caba_sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# Decode predictions using label_dict\n",
    "decoded_predictions = []\n",
    "for i, sentence_predictions in enumerate(predictions):\n",
    "    decoded_labels = [list(label_dict.keys())[list(label_dict.values()).index(label_id)] for label_id in sentence_predictions]\n",
    "    decoded_predictions.append(decoded_labels)\n",
    "\n",
    "t_token_label=[]\n",
    "t_token_pred=[]\n",
    "# Print decoded predictions\n",
    "for i, sentence in enumerate(caba_sents):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[i])\n",
    "    t_token_label.append(t_label[i])\n",
    "    t_token = t_tokens[i]\n",
    "    ind_pred = tokens.index(t_token)\n",
    "    t_token_pred.append(decoded_predictions[ind_pred])\n",
    "    # print(\"Sentence:\", sentence)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    # print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc70e130-251c-4557-b301-08594168a66b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86\n"
     ]
    }
   ],
   "source": [
    "failure_rate, failure_count=calculate_failure_rate(t_token_label, t_token_pred)\n",
    "\n",
    "rounded_number = round(failure_rate, 2)\n",
    "print(rounded_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a17a881-4efa-4809-94ca-baa5531768c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (0.75, 3), 'indirect_object': (0.5, 1), 'double_object': (0.5, 1), 'Location_modifier': (0.0, 0), 'Negation': (0.86, 6)}\n"
     ]
    }
   ],
   "source": [
    "if test_name not in input_json_a:\n",
    "    input_json_a[test_name] = (rounded_number, failure_count)\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39553725-85df-462f-8c54-6333fd811e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive': (1.0, 9), 'relative_cause': (0.75, 3), 'indirect_object': (0.5, 1), 'double_object': (0.5, 1), 'Location_modifier': (0.0, 0), 'Negation': (0.86, 6)}\n"
     ]
    }
   ],
   "source": [
    "if test_name not in input_json_a:\n",
    "    \n",
    "    input_json_a[test_name] = (rounded_number, failure_count)\n",
    "    \n",
    "print(input_json_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df764a60-f051-466b-a8bc-e370734b55ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data_result.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for k in input_json_a: \n",
    "    test_name = k\n",
    "    failure_rate, failure_count = input_json_a[k]\n",
    "    for capability in data['capabilities'].values():\n",
    "        # print(capability)\n",
    "        tests = capability.get(\"tests\")\n",
    "            # print(tests)\n",
    "        if len(tests) == 1:\n",
    "            data_name = [k for k in tests][0]\n",
    "\n",
    "            if data_name == test_name:\n",
    "                    # capability['failure_rate'] = capability.pop('pass')\n",
    "                lr_num =  capability['failure_rate']\n",
    "                capability['failure_rate'] = {\"lr\": lr_num, \"ad\": failure_rate}\n",
    "                lr_count = capability['fail']\n",
    "                capability['fail'] = {\"lr\": lr_count, \"ad\": failure_count}\n",
    "                    # print(capability['fail'])\n",
    "        else:\n",
    "            for i in tests:\n",
    "                # print(tests[i])\n",
    "                # print(i)\n",
    "                data_name = [k for k in tests[i]][0]\n",
    "\n",
    "                if data_name == test_name:\n",
    "                    # print(\"right name\")\n",
    "                        # capability['failure_rate'] = capability.pop('pass')\n",
    "                    lr_num =  capability['tests'][i][\"failure_rate\"]\n",
    "                    tests[i][\"failure_rate\"] = {\"lr\":lr_num, \"ad\": failure_rate}\n",
    "                    lr_count = capability['tests'][i][\"fail\"]\n",
    "                    tests[i][\"fail\"] = {\"lr\":lr_count,\"ad\": failure_count}\n",
    "\n",
    "with open('data_result.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)  \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a156819-c24d-4c15-84db-9836f2772e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
